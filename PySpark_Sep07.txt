 
  Agenda (PySpark)
  -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 =====================================================================


    Pre-requisites:
    ----------------
        => SQL knowledge
        => Python language


   Spark
   -----
	-> Spark is written in Scala programming language

	-> Is a framework to be used for Big Data Analytics

	-> Spark is a unified in-memory distributed computing (aka. cluster computing) framework 

	in-memory => ability of spark to persist intermediate results in memory (RAM) and subsequent
                     tasks can directly work on these persisted partitions. 


   Spark unified stack
   -------------------

	Spark provides a consistent set of APIs working on the same execution engine to process different
	analytics work loads.

	Spark Core API		=> Low-level (RDD) API for unstructured data processing
	Spark SQL		=> batch processing of Structured data.
	Spark Streaming		=> stream data processing (real time)
	Spark MLLib		=> predictive analytics (machine learning)
	Spark GraphX		=> Graph parallel computations. 


   Spark runs on multiple cluster managers
	-> local, spark standalone scheduler, YARN, Mesos, Kubenetes. 


   Spark is a polyglot
	-> Spark applications can be written in Scala, Java, Python, R


    
   Getting started with Spark
   --------------------------

   1. Databricks Community Edition

	 Signup: https://www.databricks.com/try-databricks#account
		-> Fill in the details with valid email address
		-> Next screen click on 'Get started with Community Edition' (NOT Continue button)

	 Log-in: https://community.cloud.databricks.com/login.html


   2. Setting up PySpark development environment on your local machine.

	-> Make sure you install Anaconda distribution first.
		-> https://www.anaconda.com/download
	-> Follow the instructions given in the shared document
	        https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

 
  Spark Architecture
  ------------------


	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 



   RDD (Resilient Distributed Dataset)
   -----------------------------------

	-> The fundamental in-memory data abstraction of Spark Core API

	-> RDD is a group of ditributed in-memory partitions.
		-> A partition is a collection of objects of any type. 

        -> RDD are immutable

	-> RDD are lazily evaluated
		-> Transformations does not cause execution
		-> Actions cause execution. 
		
		
      

   Creating RDDs
   -------------

	Three ways:

	1. Create an RDD from external files.

		rddFile = sc.textFile(<dataPath>, numPartitions)
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		default Number of partitions: sc.defaultMinPartitions  (1, if cores = 1, otherwise 2)

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,3,1,2,4,5,3,5,6,7,6,8,9], 3)
		default Number of partitions: sc.defaultParallelism (= the number of cores)

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)
		

   RDD Operations
   --------------

    Two types of operations:

	1. Transformations
		-> Create an RDD
		-> Does not cause executions. Does not start jobs on the cluster.
		-> Only lineage DAGs are maintained by the driver

	2. Actions
		-> Executes the RDD
		-> Creates a physical plan and sends jobs to the cluster.


   RDD Lineage DAG
   ---------------

   RDD Lineage DAG is a logical plan maintained by the driver
   Containes all hierarchical dependencies all the way from the very first RDD
   Transformations cause the creation of RDD DAGs.  

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	    rddFile Lineage DAG: (4) rddFile sc.textFile on wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	   rddWords Lineage DAG: (4) rddWords -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddPairs = rddWords.map(lambda x: (x, 1))
	   rddPairs Lineage DAG: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	  rddWc Lineage DAG: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


  
   Spark application execution flow
   --------------------------------

	Application (identified by a SparkContext object) 
	    => Jobs (Each action commands launches a Job) 
	       => Stages (Each job can have mulitple stages, based on wide transformations) 
                  => Tasks (numTasks = numPartitions) 
                     => Transformations (Each task can have multiple transformations performed as one unit of work)


   Types of Transformations
   ------------------------

     Two types:

	1. Narrow Transformations

		-> Narrow transformations are those, where the computation of each partition depends ONLY
		   on its input partition.
		-> There is no shuffling of data.
		-> Simple and efficient
		-> Input and output RDD will have same number of partitions


	2. Wide Transformations

		-> In wide transformations, the computation of a single partition depends on multiple
		   partitions of its input RDD.
		-> Data shuffle across partitions will happen.
		-> Complex and expensive
		-> Input and output RDD can have different number of partitions


   RDD Persistence
   ---------------

	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(....)
	rdd3 = rdd1.t3(....)
	rdd4 = rdd3.t4(....)
	rdd5 = rdd3.t5(....)
	rdd6 = rdd5.t6(....)
	rdd6.persist(StorageLevel.MEMORY_AND_DISK)          ---> instruction to spark to save 'rdd6' partitions
	rdd7 = rdd6.t7(....)

	rdd6.collect()

	Lineage DAG of rdd6:  (4) rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		tasks: [sc.textFile, t3, t5, t6] ==> collect

	rdd7.collect()

	Lineage DAG of rdd7:  (4) rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		tasks: [t7] ==> collect

	rdd6.unpersist()


    Storage Levels
    ---------------	
      1. MEMORY_ONLY  		=> default, Memory Serialized 1x Replicated
      2. MEMORY_AND_DISK   	=> Disk Memory Serialized 1x Replicated
      3. DISK_ONLY		=> Disk Serialized 1x Replicated
      4. MEMORY_ONLY_2	        => Memory Serialized 2x Replicated
      5. MEMORY_AND_DISK_2      => Disk Memory Serialized 2x Replicated


    Commands
    ---------
	 1. rdd1.cache()    					-> in-memory persistence
	 2. rdd1.persist()					-> in-memory persistence
	 3. rdd1.persist(StorageLevel.MEMORY_AND_DISK)   

	 4. rdd1.unpersist()
	

   RDD Transformations
   -------------------
   
   1. map 		P: U -> V
			object to object transformation
			input RDD: N objects, output RDD: N objects
	
		rddFile.map(lambda x: x.split(" ") ).collect()


   2. filter		P: U -> Boolean	
			Filters the objects based on the function	
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x) > 51).collect()


  3. glom		P: None
			Returns one list object per partition

		rdd1		    rdd2 = rdd1.glom()	
		P0: 2,3,1,4,6  -> glom -> P0: [2,3,1,4,6]
		P1: 5,6,7,4,3  -> glom -> P1: [5,6,7,4,3]
		P2: 8,9,0,4,7  -> glom -> P2: [8,9,0,4,7]
	
		rdd1.glom().map(sum).collect()


  4. flatMap           	P: U -> Iterable[V] 
			flatmap flattens the iterables produced by the function 

		rddWords = rddFile.flatMap(lambda x: x.split())


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: [ sum(p) ] ).glom().collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).glom().collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions but we get partition-index as additional function input parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : 
		     map(lambda x: (i, x*10), p)).filter(lambda x: x[0] == 1).collect()


  7. distinct		P: None, Optional: numPartition
			Returns the distict objects of the RDD. 

		rddWords.flatMap(lambda x: x).distinct().collect()	


   Types of RDDs
   -------------
	1. Generic RDD    =>  RDD[U]             
	2. Pair RDD	  =>  RDD[(K, V)]        


   8. mapValues		P: U -> V
			Applied only on Pair-RDDs
			Transforms only the 'value part' of the (K,V) pairs

		rdd3 = rdd2.mapValues(lambda x: (x, x+5))


   9. sortBy		P: U -> V, optional: ascending (True/False), numPartition
			Sorts the objects of the RDD based on the function's output.

		rdd1.sortBy(lambda x: x%3).glom().collect()
		rdd1.sortBy(lambda x: x%3, False).glom().collect()
		rdd1.sortBy(lambda x: x%3, True, 5).glom().collect()


  10. groupBy 		P: U -> V
			Returns a Pair RDD, where
				key: Each unique value of the function output
				value: ResultIterable, grouped objects that produced the key.

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split()) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)

  11. partitionBy 	P: numPartitions, Optional: partitioning-function (default: hash)
			Applied only on Pair RDDs
			Partitioning is done based on the key

  		rdd4 = rdd1 \
			.map(lambda x: (x, 2423)) \
			.partitionBy(3, hash) \
			.map(lambda x: x[0])


  12. repartition	P: numPartition
			Is used to increase or decrease the number of partitions of the output RDD
			Cause global shuffle

		rdd2 = rdd1.repartition(5)


  13. coalesce		P: numPartition
			Is used to only decrease the number of partitions of the output RDD
			Causes partition merging  

		rdd2 = rdd1.repartition(3)



        Recommendations
        ---------------
	1. The size of each partition should be between 100 MB and 1 GB.
           If you are running on top of Hadoop, ideally use 128 MB partition. 

  	2. The number of partitions should be a multiple of total number of cores allocated. 
	   If the number of partition is less than but close to 2000, but it up to 2000. 

        3. The number of CPU cores per executor should be 5 (in case of YARN & Kubernetes)


  14. union		Performs union of two RDDs
			The output RDD will simply add all the partitions of the input RDDs.

		rdd12 = rdd10.union(rdd11)


  ..ByKey Transformations
  -----------------------
    -> Are wide transformations
    -> Are applied only on Pair RDDs. 

  
  15. sortByKey		P: None, Optional: ascending (True/False), numPartition
			The output RDD is sorted based on the Key.
			Are applied only on Pair RDDs.
	
		rdd2.sortByKey().glom().collect()
		rdd2.sortByKey(False).glom().collect()
		rdd2.sortByKey(False, 6).glom().collect()


  16. groupByKey	P: None, Optional: numPartition
			Returns a Pair RDD where
				Key: Each unique key
				Value: ResultIterable containing grouped values.

			CAUTION: Avoid groupByKey if possible. 
			
   
		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False, 1)


  17. reduceByKey	P: (U, U) -> U
			Reduces all the 'values' of 'each unique-key' within each partition and then across partitions


		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False, 1)



  RDD Actions
  ------------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) => U
			It reduces the entire RDD to one obejct of the same type by iterativly applying
			the function to each partition in the first stage and across partitions in the
  			next stage. 


		rdd1
		P0: 3,2,4,5,7,8 -> reduce ->  -23 -> reduce -> 6
		P1: 6,9,5,9,0,1 -> reduce ->  -18
		P2: 4,6,2,1,0,6 -> reduce ->  -11

		rdd1.reduce(lambda x, y: x - y)
		  
		rdd2.reduce(lambda x, y : (x[0] + y[0], x[1] + y[1]) )
                -> here rdd2 is a Pair RDD with numbers.

  5. take(n)	-> returns a list with first n objects of the RDD.

		rdd1.take(10)

  6. takeOrdered(n, [fn]) -> returns a list with first n sorted objects of the RDD.
		
		rddWords.takeOrdered(25)
		rddWords.takeOrdered(25, len)

  7. takeSample(True/False, n, [seed])   

		rdd1.takeSample(True, 20)         => with-replacement sampling
		rdd1.takeSample(True, 20, 6464)   => 6464 is a seed

		rdd1.takeSample(False, 20)	  => with-out-replacement sampling
		rdd1.takeSample(False, 20, 6464)

  8. countByValue

  9. countByKey

  10. foreach	=> does not return anything
		   runs a function on all the objects of the RDD.

		rddWc.foreach(lambda x : print("key: {}, value: {}".format(x[0], x[1]) ))

   Use-Case
   --------
    dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

    From cars.tsv, find the average-weight of all models of each make of American-origin cars
	-> Arrange the data in the DESC order of average-weight
	-> Save the output in a single text file. 

        => Try it yourself


  Spark Closures
  --------------

   In Spark, a closure constitutes all the variables and methods which must be visible for 
   the executor to perform its computations on the RDD. 

   This closure is serialized and a separate copy is sent to each executor.


        c = 0

	def isPrime(n):
	   return True if n is prime
	   else return False

	def f1(n):
	   global c
	   if (isPrime(n)) c += 1
	   return n * 2


	rdd1 = sc.parallelize(range(1, 4001), 4)
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c )    # 0 


      Limitation: We can not use local variables to implement global counter
      Solution: Use accumulator variable


  Spark Shared Variables
  ----------------------

   1. Accumulator variable

	c = sc.accumulator(0)

	def isPrime(n):
	   return True if n is prime
	   else return False

	def f1(n):
	   global c
	   if (isPrime(n)) c.add(1)
	   return n * 2


	rdd1 = sc.parallelize(range(1, 4001), 4)
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c.value ) 



   2. Broadcast variable

	d = {1: a, 2: b, 3: c, 4: d, 5: e, 6: f, .......}   # 100 MB
        bc = sc.broadcast(d)

	def f1(k):
	   global bc
	   return bc.value[k]


	rdd1 = sc.parallelize([1,2,3,4,5, ....], 4)
	rdd2 = rdd1.map( f1 )

	rdd2.collect()    # [a, b, c, ...]



  Spark Submit
  ------------

	Spark Submit is a single command to submit any Spark application to any cluster manager. 


	spark-submit [options] <app jar | python file | R file> [app arguments]
	
	spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--executor-memory 5G
		--driver-cores 2
		--executor-cores 5
		--num-executors 10
		E:\PySpark\spark_core\examples\wordcount.py [app args]


	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt sample_out 1

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py



  ===================================================
    	Spark SQL (pyspark.sql)
  ===================================================

   -> High level API built on top of Spark Core.

   -> Spark's structured data processing API. 

	   File Formats: Parquet (default), ORC, JSON, CSV (delimited text), Text
	   JDBC Format : Any RDBMS or NoSQL
	   Hive Format : Process the data from Hive warehouse.

  SparkSession
  ------------
	-> SparkSession is the staring point of execution for any Spark SQL app. 
	-> SparkSession represents a user-session inside an application

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate()  


  DataFrames (DF)
  ---------------
	DataFrame is a collection of distributed in-memory partitions that are immutable and lazily evaluated. 
	DataFrame is a collection 'Row' objects

	DataFrame has two components:
		Data	: Collection of partitions containing 'Row' object
		Schema  : StructType object

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)



  Basic steps in a Spark SQL program
  ----------------------------------

   1. Load the data from some data source into a DataFrame

	df1 = spark.read.format("json").load(inputFilePath)
	df1 = spark.read.load(inputFilePath, format="json")
	df1 = spark.read.json(inputFilePath)


   2. Transform the DF using DF Transformation methods or using SQL

		Using DF Transformation methods
		-------------------------------
			df2 = df1.select("userid", "name", "age", "gender", "phone") \
				 .where("age is not null") \
				 .orderBy("gender", "age") \
				 .groupBy("age").count() \
				 .limit(4)

			df2.show()

		Using SQL
		---------
			df1.createOrReplaceTempView("users")
			spark.catalog.listTables()

			qry = """select age, count(1) as count
				from users
				where age is not null
				group by age
				order by age
				limit 4"""
					
			df3 = spark.sql(qry)
			df3.show()

   3. Save the DataFrame some external structured destination.  
		
	df3.write.format("json").save(outputPath)
	df3.write.save(outputPath, format="json")
	df3.write.json(outputPath) 
	df3.write.mode("Overwrite").json(outputPath) 


  Save Modes
  ----------
    -> Control the behaviour when writing to existing directory

	ErrorIfExists (default)
	Ignore
	Append
	Overwrite

	=> df3.write.json(outputPath, mode="Overwrite") 
	   df3.write.mode("Overwrite").json(outputPath) 

  
  LocalTempViews & GlobalTempViews
  --------------------------------
	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")

	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView ("gusers")


  DataFrame Transformations
  -------------------------

  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME",
			"DEST_COUNTRY_NAME",
			"count")

	df2 = df1.select(
		col("ORIGIN_COUNTRY_NAME").alias("origin"),
		column("DEST_COUNTRY_NAME").alias("destination"),
		expr("count").cast("int"),
		expr("count + 10 as newCount"),
		expr("count > 200 as highFrequency"),
		expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
	      )


  2. where / filter

	df3 = df2.where("count > 500 and domestic = false")
	df3 = df2.filter("count > 500 and domestic = false")

	df3 = df2.filter( col("count") > 500 )

	df3.show()


  3. orderBy

	df3 = df2.orderBy("count", "origin")
	df3 = df2.sort("count", "origin")

	df3 = df2.sort(desc("count"), asc("origin"))
	df3 = df2.sort(col("count").desc(), col("origin").asc())

  
  4. groupBy => returns a "pyspark.sql.group.GroupedData" object
		apply an aggregation method to return a DataFrame. 


	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
         	.agg( count("count").alias("count"),
              	      sum("count").alias("sum"),
                      max("count").alias("max"),
                      round(avg("count"), 2).alias("avg")
                   )


  5. limit
		df2 = df1.limit(10)


  6. selectExpr

	df2 = df1.selectExpr(
        	"ORIGIN_COUNTRY_NAME as origin",
        	"DEST_COUNTRY_NAME as destination",
        	"count",
        	"count + 10 as newCount",
        	"count > 200 as highFrequency",
        	"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"
       	     )


  7. withColumn & withColumnRenamed

	
		df3 = df1.withColumn("newCount", col("count") + 10) \
			.withColumn("highFrequency", expr("count > 200")) \
			.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
			.withColumn("count", col("count").cast("int")) \
			.withColumn("hundred", lit(10 * 10) ) \
			.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
			.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

  		-----------------------------------------------------------

		ageGroupDf = userDf.withColumn("ageGroup", when( col("age") < 13, "Child")
                                          		.when( col("age") < 20, "Teenager")
                                          		.when( col("age") < 60, "Adult")
                                          		.otherwise("Senior") )


  8. udf (user defined fuunction)



	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"
		
	get_age_group = udf(getAgeGroup, StringType())  

	ageGroupDf = userDf.withColumn("ageGroup", get_age_group(col("age")) )

	-------------------------------------

	@udf(returnType = StringType())
	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"
		
	ageGroupDf = userDf.withColumn("ageGroup", getAgeGroup(col("age")) )

	-------------------------------------

	spark.catalog.listFunctions()

	spark.udf.register("get_age_group", getAgeGroup, StringType())

	qry = "select id, name, age, get_age_group(age) as ageGroup from users"

	ageGroupDf2 = spark.sql(qry)


  9. drop	=> exclude one or more columns in the output dataframe.

	df3 = df2.drop("hundred", "newCount")
	df3.show(5)
	df3.printSchema()


  10. dropna      => drop the rows with 'null' values in all or specified columns of the input DF. 

	usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
	usersDf.show()

	df3 = usersDf.dropna()
	df3 = usersDf.dropna(subset=["phone", "age"])

	df3.show()


  11. dropDuplicates

	listUsers = [(1, "Raju", 5),
				 (1, "Raju", 5),
				 (3, "Raju", 5),
				 (4, "Raghu", 35),
				 (4, "Raghu", 35),
				 (6, "Raghu", 35),
				 (7, "Ravi", 35)]

	userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	userDf.show()

	df3 = userDf.dropDuplicates(["name","age"])
	df3.show()


  12. distinct
	
	 userDf.distinct().show()


	''' Q: How many unique DEST_COUNTRY_NAMEs are there in the DF ? '''

		df1.select("DEST_COUNTRY_NAME").distinct().count()
		df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()
		df1.groupBy("DEST_COUNTRY_NAME").count().count()


  13. randomSplit	
          
            -> list of weights as a parameter
	    -> returns a ist of dataframes split in the specified weights randomly. 
		

	df2, df3, df4 = df1.randomSplit([0.4, 0.3, 0.3], 575)
	print(df2.count(),  df3.count(),  df4.count())	


  14. sample	
	   -> Sample types: With-replacement, with-out-replacement

		df2 = df1.sample(True, 0.55)		-> With-replacement
		df2 = df1.sample(True, 1.55)		-> fraction > 1 is allowed
		df2 = df1.sample(True, 0.55, 435)	-> 435 is a seed

		df2 = df1.sample(False, 0.55)		-> With-out-replacement
		df2 = df1.sample(False, 1.55)		-> ERROR: fraction > 1 is NOT allowed
		df2 = df1.sample(False, 0.55, 435)	-> 435 is a seed	

		df2.show()
		df2.count()


  15. union, intersect, subtract

		df3 = df1.where("DEST_COUNTRY_NAME = 'India'")
		df3.show()   # 1 rows
		df3.printSchema()
		df3.rdd.getNumPartitions()

		df4 = df3.union(df2)
		df4.show()
		df4.printSchema()


		df5 = df4.subtract(df3)
		df5.show()

		df6 = df4.intersect(df3)
		df6.show()


        default shuffle partitions
        ---------------------------
	spark.conf.get("spark.sql.shuffle.partitions")
	spark.conf.set("spark.sql.shuffle.partitions", "5")


  16. repartition

		df2 = df1.repartition(4)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(2)
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(2, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df5.rdd.getNumPartitions()	


  17. coalesce

		df6 = df2.coalesce(2)
		df6.rdd.getNumPartitions()

 
  18. join  => discussed as a separate topic. 

   

  Create a RDD from DataFrame
  ---------------------------
	rdd1 = df1.rdd
	rdd1.collect()


  Create a DataFrame fom programmatic data
  ----------------------------------------

	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers)
	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])

	mySchema = "id INT, name STRING, age INT"
	df2 = spark.createDataFrame(listUsers, schema=mySchema)


  Create a DataFrame fom RDD
  ---------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df2 = rdd1.toDF(["id", "name", "age"])


  Applying programmatic schema to a DataFrame
  -------------------------------------------
  	mySchema = "id INT, name STRING, age INT"
	df2 = rdd1.toDF(schema = mySchema)
	----------------------------------
	mySchema = StructType([
				StructField("id", IntegerType(), True),
				StructField("name", StringType(), True),
				StructField("age", IntegerType(), True)
			]) 

	df2 = rdd1.toDF(schema = mySchema)
	---------------------------------
	mySchema = StructType([
				StructField("userid", IntegerType(), True),
				StructField("name", StringType(), True),
				StructField("age", IntegerType(), True),
				StructField("gender", StringType(), True),
				StructField("phone", StringType(), True)
			]) 

	df1 = spark.read.schema(mySchema).json(inputFilePath)
	df1 = spark.read.json(inputFilePath, schema=mySchema)
      
	

   Working with different file formats
   -----------------------------------
   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)	


  CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")


  Text
	read
		df1 = spark.read.text(inputPath)

	write
		df1.write.text(outputPath)

   

  
   DataFrame Joins
   ---------------


    Supported Joins:  inner, left, right, full, left_semi, left_anti, cross

    left_semi join
    --------------

	Is like inner join, but gets data from only the left table. 

	Equivalent to the following sub-query:
		
		select * from emp where deptid in (select id from dept) 

    left-anti join
    --------------
	Equivalent to the following sub-query:
		
		select * from emp where deptid not in (select id from dept)
	
	
     Using SQL method
     ----------------
		employee.createOrReplaceTempView("emp")
		department.createOrReplaceTempView("dept")

		qry = """select emp.*
				 from emp left anti join dept
				 on emp.deptid = dept.id"""
				 
		joinedDf = spark.sql(qry)         

		joinedDf.show()


     Using DF transformation
     -----------------------
		joinCol = employee.deptid == department.id

		joinedDf = employee.join(department, joinCol, "left_anti")

		joinedDf.show()


  
   Use-Case
   --------

     datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

     From movies.csv and ratings.csv data files fetch the top 10 movies with heighest average user-rating.
	-> Consider only those movies that have atleast 30 user-rating.
	-> Data to be fetched: movieId, title, totalRatings, averageRating
	-> Sort the data in the descending order of averageRating
	-> Save the output to a single pipe-separated CSV file with header. 
	-> Use DataFrame transformations approach. 

        => Try it yourself..


   JDBC format - Working with MySQL
   --------------------------------
import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


     Hive format: Working with Hive Warehouse
     ----------------------------------------
	
# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()  

spark.catalog.currentDatabase()
spark.catalog.listDatabases()

spark.sql("show databases").show()

spark.sql("drop database sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")
spark.sql("use sparkdemo")

spark.catalog.listTables()
spark.catalog.currentDatabase()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
 
summaryDf2.write \
   .mode("overwrite") \
   .format("hive") \
   .saveAsTable("topRatedMovies")
   
   
spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()



 ===========================================
   Spark Streaming (Structured Streaming) 
 ===========================================

   Spark Streaming APIs:

	-> Spark Streaming (DStreams API) built on top of Spark Core
		-> pyspark.streaming
		-> Old & legacy API

	-> Structured Streaming built on top of Spark SQL 
		-> pyspark.sql.streaming
		-> Current and preferred API

  Notes:

  The key idea in Structured Streaming is to treat a live data stream as a table that is being 
  continuously appended.

  You express your streaming computation as standard batch-like query as on a static table, 
  and Spark runs it as an incremental query on the unbounded input table.

  Programming Model
  -----------------
	A query on the input will generate the “Result Table”.

	Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	which eventually updates the Result Table.

	Whenever the result table gets updated, we would want to write the changed result rows 
	to an external sink.

	Structured Streaming does not materialize the entire table.

	It reads the latest available data, processes it incrementally to update the result, and 
	then discards the source data.

	Spark is responsible for updating the Result Table when there is new data, thus relieving 
	the users from reasoning about fault-tolerance and data consistency.


  Sources
  -------

	Socket Stream (host & port)	
	File Stream (directory) => text, csv, json, parquet, orc
	Rate stream
	Kafka Stream


  Sinks
  -----

	Console Sink
	File Sinks  (directory) => text, csv, json, parquet, orc
	Kafka sink
	ForEachBatch sink
	ForEach sink 
	Memory sink

  
   



    

    








































